# -*- coding: utf-8 -*-
"""combine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11oyWAXdW2eOFd17v55YlSOHT2i0bIrvt
"""

import os
import sys
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from keras.callbacks import *
from imblearn.over_sampling import RandomOverSampler
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from tensorflow import keras
from keras.models import Model, Sequential
from keras.layers import *
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

metadata = '/content/HAM10000_metadata.csv'
hmnist_8_8_RGB = '/content/hmnist_8_8_RGB.csv'
hmnist_28_28_RGB = '/content/hmnist_28_28_RGB.csv'
hmnist_8_8_L = '/content/hmnist_8_8_L.csv'
hmnist_28_28_L = '/content/hmnist_28_28_L.csv'

classes111 = {4: ('nv', ' melanocytic nevi'),
              6: ('mel', 'melanoma'),
              2: ('bkl', 'benign keratosis-like lesions'),
              1: ('bcc', ' basal cell carcinoma'),
              5: ('vasc', ' pyogenic granulomas and hemorrhage'),
              0: ('akiec', 'Actinic keratoses and intraepithelial carcinomae'),
              3: ('df', 'dermatofibroma')}

df111 = pd.read_csv(hmnist_28_28_RGB, delimiter=',')
df111.dataframeName = 'hmnist_28_28_RGB.csv'
nRow111, nCol111 = df111.shape
print(f'There are {nRow111} rows and {nCol111} columns')

label111 = df111["label"]
data111 = df111.drop(columns=["label"])
data111.head()

import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import RandomOverSampler
import numpy as np
from sklearn.model_selection import train_test_split
import os, cv2
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D
from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report

import pandas as pd
data1112 = pd.read_csv('/content/hmnist_28_28_RGB.csv')

yk1112 = data1112['label']
xk1112 = data1112.drop(columns=['label'])
tabular_data1112 = pd.read_csv('/content/HAM10000_metadata.csv')
tabular_data1112.head()

tabular_data1112.columns

classes1112 = {4: ('nv', ' melanocytic nevi'), 6: ('mel', 'melanoma'), 2: ('bkl', 'benign keratosis-like lesions'), 1: ('bcc', ' basal cell carcinoma'), 5: ('vasc', ' pyogenic granulomas and hemorrhage'), 0: ('akiec', 'Actinic keratoses and intraepithelial carcinomae'), 3: ('df', 'dermatofibroma')}

sns.countplot(x='dx', data=tabular_data1112)
plt.xlabel('Disease', size=12)
plt.ylabel('Frequency', size=12)
plt.title('Frequency Distribution of Classes', size=16)
plt.show()

bar1112, ax11111 = plt.subplots(figsize=(10, 10))
plt.pie(tabular_data1112['sex'].value_counts(), labels=tabular_data1112['sex'].value_counts().index, autopct="%.1f%%")
plt.title('Gender of Patient', size=16)
plt.show()

bar1112, ax112 = plt.subplots(figsize=(10, 10))
sns.histplot(tabular_data1112['age'])
plt.title('Histogram of Age of Patients', size=16)
plt.show()

value1112 = tabular_data1112[['localization', 'sex']].value_counts().to_frame()
value1112.reset_index(level=[1, 0], inplace=True)
temp1112 = value1112.rename(columns={'localization': 'location', 0: 'count'})
bar1112, ax112 = plt.subplots(figsize=(12, 12))
sns.barplot(x='location', y='count', hue='sex', data=temp1112)
plt.title('Location of disease over Gender', size=16)
plt.xlabel('Disease', size=12)
plt.ylabel('Frequency/Count', size=12)
plt.xticks(rotation=90)
plt.show()

def plotScatterMatrix(df, plotSize, textSize):
    df = df.select_dtypes(include=[np.number])
    df = df.dropna('columns')
    df = df[[col for col in df if df[col].nunique() > 1]]
    columnNames = list(df)
    if len(columnNames) > 10:
        columnNames = columnNames[:10]
    df = df[columnNames]
    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')
    corrs = df.corr().values
    for i, j in zip(*plt.np.triu_indices_from(ax, k=1)):
        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)
    plt.suptitle('Scatter and Density Plot')
    plt.show()

plotScatterMatrix(data1112, 20, 10)

def plotScatterMatrix(df, plotSize, textSize):
    df = df.select_dtypes(include=[np.number])
    df = df.dropna('columns')
    df = df[[col for col in df if df[col].nunique() > 1]]
    columnNames = list(df)
    if len(columnNames) > 10:
        columnNames = columnNames[:10]
    df = df[columnNames]
    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')
    corrs = df.corr().values
    for i, j in zip(*np.triu_indices_from(ax, k=1)):
        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)
    plt.suptitle('Scatter and Density Plot')
    plt.show()

# our dataset is imbalanced, so we try to balance the data using oversample the data using random points
oversample111 = RandomOverSampler()
data111, label111 = oversample111.fit_resample(data111, label111)
data111 = np.array(data111).reshape(-1, 28, 28, 3)
label111 = np.array(label111)
y11111 = [7000, 7000, 7000, 7000, 7000, 7000, 7000]
print(type(label111))
data111.shape

X_train111, X_test111, y_train111, y_test111 = train_test_split(data111, label111, test_size=0.2, random_state=42)

def plotImg111(img):
    plt.imshow(img)
    plt.title("Skin Cancer Image")
    plt.grid(False)
    plt.axis("off")
    plt.show()
plotImg111(X_train111[0])

fig111, axes111 = plt.subplots(5, 5)
fig111.set_size_inches(10, 10)
for i in range(5):
    for j in range(5):
        n = np.random.randint(0, 1000, 1)
        axes111[i, j].imshow(X_train111[n].reshape(28, 28, 3))
    plt.tight_layout()

import matplotlib.pyplot as plt

# Create a list of disease classes as x-axis labels
x11111 = ["bkl", "nv", "df", "mel", "vasc", "bcc", "akiec"]
plt.bar(x11111, y11111)
plt.title("Frequency Distribution of Classes")
plt.xlabel("Disease")
plt.ylabel("Frequency")

plt.show()

import cv2
import numpy as np
import pandas as pd
import os
import pandas as pd
from scipy.stats import kurtosis
from scipy.stats import skew
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE
from scipy.stats import moment
from skimage.feature import graycomatrix, graycoprops

test = pd.read_csv('/content/hmnist_28_28_RGB.csv')

fe=[]
test.head(10)
X = test.iloc[:,0:-1]
Y = test.iloc[:,-1]

from collections import Counter
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=0)
X, Y = ros.fit_resample(X, Y)
X.shape, Y.shape
X = np.array(X)
Y = np.array(Y)
X = X.reshape(X.shape[0],28,28,3)

for i in range(46935):
 a=X[i,:,:,:]
 a = np.array(a, dtype=np.uint8)
 X[i,:,:,:]=cv2.cvtColor(a, cv2.COLOR_BGR2RGB)

for i in range(46935):
 a=X[i,:,:,:]
 ar=a[:,:,0]
 ag=a[:,:,1]
 ab=a[:,:,2]
 f1=skew(ar,axis=None)
 f2=kurtosis(ar,axis=None)
 f3=ar.mean()
 f4=ar.std()
 f5=moment(ar,moment=5,axis=None)
 f6=skew(ag,axis=None)
 f7=kurtosis(ag,axis=None)
 f8=ag.mean()
 f9=ag.std()
 f10=moment(ar,moment=5,axis=None)
 f11=skew(ab,axis=None)
 f12=kurtosis(ab,axis=None)
 f13=ab.mean()
 f14=ab.std()
 f15=moment(ar,moment=5,axis=None)
 a=X[i,:,:,:]
 a = np.array(a, dtype=np.uint8)
 a=cv2.cvtColor(a, cv2.COLOR_BGR2GRAY)
 f16=skew(a,axis=None)
 f17=kurtosis(a,axis=None)
 f18=a.mean()
 f19=a.std()
 f20=moment(a,moment=5,axis=None)
 glcm = graycomatrix(a,distances=[1], angles=[0, 45, 90], levels=256,
                    symmetric=True, normed=True)
 f21=graycoprops(glcm, 'dissimilarity')[0][0]
 f22=graycoprops(glcm, 'correlation')[0][0]
 f23=graycoprops(glcm, 'contrast')[0][0]
 f24=graycoprops(glcm, 'energy')[0][0]
 f25=graycoprops(glcm,'homogeneity')[0][0]
 glcm = graycomatrix(ab,distances=[1], angles=[0, 45, 90], levels=256,
                        symmetric=True, normed=True)
 f26=graycoprops(glcm, 'dissimilarity')[0][0]
 f27=graycoprops(glcm, 'correlation')[0][0]
 f28=graycoprops(glcm, 'contrast')[0][0]
 f29=graycoprops(glcm, 'energy')[0][0]
 f30=graycoprops(glcm,'homogeneity')[0][0]
 glcm = graycomatrix(ag,distances=[1], angles=[0, 45, 90], levels=256,
                        symmetric=True, normed=True)
 f31=graycoprops(glcm, 'dissimilarity')[0][0]
 f32=graycoprops(glcm, 'correlation')[0][0]
 f33=graycoprops(glcm, 'contrast')[0][0]
 f34=graycoprops(glcm, 'energy')[0][0]
 f35=graycoprops(glcm,'homogeneity')[0][0]
 f=[f1,f2,f3,f4,f5,f6,f7,f8,f9,f10,f11,f12,f13,f14,f15,f21,f22,f23,f24,f25]
 fe.append(f)
fe=np.asarray(fe)

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Feature extraction
model = LogisticRegression()
rfe = RFE(estimator=model, n_features_to_select=15)
fit = rfe.fit(fe, Y)

print("Num Features: %s" % (fit.n_features_))
print("Selected Features: %s" % (fit.support_))
print("Feature Ranking: %s" % (fit.ranking_))

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
fe = scaler.fit_transform(fe)

trainX,testX,trainY,testY = train_test_split(X,Y,test_size=0.2,random_state=17)
trainX,validX,trainY,validY = train_test_split(trainX,trainY,test_size=0.2,random_state=17)
ftrainX,ftestX,ftrainY,ftestY = train_test_split(fe,Y,test_size=0.2,random_state=17)
ftrainX,fvalidX,ftrainY,fvalidY = train_test_split(ftrainX,ftrainY,test_size=0.2,random_state=21)
trainX = trainX.astype('float64') / 255.0
testX =  testX.astype('float64') / 255.0
trainY = to_categorical(trainY)
testY = to_categorical(testY)

validX = validX.astype('float64') / 255.0
validY = to_categorical(validY)

from keras.layers import Flatten, Dense, Activation, Conv2D, MaxPooling2D, Dropout, BatchNormalization
from keras.models import load_model, Sequential
from keras.callbacks import EarlyStopping,ReduceLROnPlateau, ModelCheckpoint
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import GaussianNoise
from keras.layers import Dropout

def plot_model_history(model_history):
    fig, axs = plt.subplots(1,2,figsize=(15,5))
    # summarize history for accuracy
    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])
    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy')
    axs[0].set_xlabel('Epoch')
    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)
    axs[0].legend(['train', 'val'], loc='best')
    # summarize history for loss
    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])
    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss')
    axs[1].set_xlabel('Epoch')
    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)
    axs[1].legend(['train', 'val'], loc='best')
    plt.show()
    plot_model_history(history)

import matplotlib.pyplot as plt
from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate
from keras.models import Model
import numpy as np



del model
img_input = Input(shape=(28, 28, 3))  ## branch 1 with image input
x = Conv2D(64, (3, 3),activation='relu')(img_input)
x=MaxPooling2D(2,2)(x)
x = Dropout(0.1)(x)
x = Conv2D(128, (3, 3),activation='relu')(x)
x=MaxPooling2D(2,2)(x)
x = Dropout(0.3)(x)
x = Conv2D(128, (3, 3),activation='relu')(x)
x = Dropout(0.2)(x)
x=BatchNormalization()(x)
x = Conv2D(256, (3, 3),activation='relu')(x)
x = Dropout(0.4)(x)
x=BatchNormalization()(x)
x = Flatten()(x)
out_a = Dense(7)(x)

num_input = Input(shape=(20,))        ## branch 2 with numerical input
x1 = Dense(40, activation='relu')(num_input)
x1 = Dropout(0.3)(x1)
x1= Dense(80, activation='relu')(x1)
x1 = Dropout(0.4)(x1)
x1= Dense(100, activation='relu')(x1)
x1 = Dropout(0.3)(x1)
x1= Dense(200, activation='relu')(x1)
x1 = Dropout(0.5)(x1)
x=BatchNormalization()(x)
out_b = Dense(7)(x1)

concatenated = concatenate([out_a,out_b])
out = Dense(7, activation='softmax')(concatenated)

model = Model([img_input,num_input,], out)
print(model.summary())

import matplotlib.pyplot as plt
from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate
from keras.models import Model
import numpy as np

################## ADAM OPTIMIZER   #######################3

from keras.optimizers import Adam
optimizer = Adam(lr=0.00125, epsilon = 1e-8, beta_1 = .9, beta_2 = .999)
# Compile the model
model.compile(optimizer = optimizer , loss = "categorical_crossentropy", metrics=["accuracy"])
# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
                                            patience=2,
                                            verbose=1,
                                            factor=0.5,
                                            min_lr=0.0001)

### Just for sanity check
trainX1=np.concatenate((trainX,trainX,trainX), axis=0)
ftrainX1=np.concatenate((ftrainX,ftrainX,ftrainX), axis=0)
trainY1=np.concatenate((trainY,trainY,trainY), axis=0)
X1 = [trainX1,ftrainX1]
y1 = trainY1
X2 = [validX,fvalidX]
y2 = validY
testX=np.concatenate((trainX,validX,testX), axis=0)
ftestX=np.concatenate((ftrainX,fvalidX,ftestX), axis=0)
testY=np.concatenate((trainY,validY,testY), axis=0)
X3 = [testX,ftestX]
y3 = testY

history=model.fit(X1, y1,batch_size=128, epochs=100, verbose=1, validation_data = (X2,y2), callbacks=[learning_rate_reduction])
pred=np.round(model.predict(X3),0)

import pandas as pd

# Assuming 'history' contains the training history
# history = model.fit(...)

# Create a DataFrame from the training history
history_df = pd.DataFrame(history.history)

# Display the DataFrame
print(history_df)

# Extract validation accuracy, validation loss, and loss from the DataFrame
validation_accuracy = history_df['val_accuracy'].max()
validation_loss = history_df['val_loss'].min()
final_loss = history_df['loss'].iloc[-1]

# Display the results
print(f"\nValidation Accuracy: {validation_accuracy:.4f}")
print(f"Validation Loss: {validation_loss:.4f}")
print(f"Final Loss: {final_loss:.4f}")

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

print("Confusion Matrix:")
print(conf_mat)

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

# Extract TP, TN, FP, FN
TP = conf_mat[1, 1]  # True Positive
TN = conf_mat[0, 0]  # True Negative
FP = conf_mat[0, 1]  # False Positive
FN = conf_mat[1, 0]  # False Negative

# Display the results
print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1)))
print(classification_report(y3,pred))
plt.plot(history.history['accuracy'],'g--', linewidth=2, markersize=6)
plt.plot(history.history['val_accuracy'],'^k:', linewidth=2, markersize=6)

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train accuracy', 'test accuracy'], loc='upper left')
plt.grid()
plt.show()
# summarize history for loss
plt.plot(history.history['loss'],'b--', linewidth=2, markersize=6)
plt.plot(history.history['val_loss'],'^k:', linewidth=2, markersize=6)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train loss', 'test loss'], loc='upper left')
plt.grid()
plt.show()

score = model.evaluate(X3, y3,verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

!pip install scipy
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
from itertools import cycle
from scipy.interpolate import interp1d

x_test = X3
n_classes=7
lw=1
y_score =pred

### MACRO
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y3[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y3.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
# Plot ROC curves for the multiclass problem

# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
  mean_tpr += interp1d(fpr[i], tpr[i])(all_fpr)

# ... (rest of the code)

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=1)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=1)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve for skin cancer classification')
plt.legend(loc="lower right")
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10,10))


ax = sns.heatmap(conf_mat, annot=True, cmap='Blues', linewidths=.9, ax=ax)

ax.set_title('Confusion Matrix with labels\n\n');

ax.set_xlabel('\nPredicted Values')

ax.set_ylabel('Actual Values ');

ax.xaxis.set_ticklabels(['0','1','2','3','4','5','6'])

ax.yaxis.set_ticklabels(['0','1','2','3','4','5','6'])

plt.show()

import numpy as np

def calculate_metrics(conf_matrix):
    metrics = []

    for i in range(conf_matrix.shape[0]):
        true_positive = conf_matrix[i, i]
        false_positive = np.sum(conf_matrix[:, i]) - true_positive
        false_negative = np.sum(conf_matrix[i, :]) - true_positive
        true_negative = np.sum(conf_matrix) - true_positive - false_positive - false_negative

        metrics.append({
            'class': i,
            'true_positive': true_positive,
            'true_negative': true_negative,
            'false_positive': false_positive,
            'false_negative': false_negative
        })

    return metrics

# Example usage:
# Replace `your_conf_matrix` with your actual confusion matrix of size 7x7
your_conf_matrix = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

metrics = calculate_metrics(your_conf_matrix)

# Print the metrics for each class
for metric in metrics:
    print(f"Class {metric['class']}:")
    print(f"True Positive: {metric['true_positive']}")
    print(f"True Negative: {metric['true_negative']}")
    print(f"False Positive: {metric['false_positive']}")
    print(f"False Negative: {metric['false_negative']}\n")

#################### NADAM OPTIMIZER ##############################

from keras.optimizers import Nadam
from keras.callbacks import ReduceLROnPlateau

# Change optimizer to Nadam
optimizer = Nadam(lr=0.00125, epsilon=1e-8, beta_1=0.9, beta_2=0.999)

# Compile the model
model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])

# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(
    monitor='val_acc',
    patience=2,
    verbose=1,
    factor=0.5,
    min_lr=0.0001
)

### Just for sanity check
trainX1 = np.concatenate((trainX, trainX, trainX), axis=0)
ftrainX1 = np.concatenate((ftrainX, ftrainX, ftrainX), axis=0)
trainY1 = np.concatenate((trainY, trainY, trainY), axis=0)
X1 = [trainX1, ftrainX1]
y1 = trainY1
X2 = [validX, fvalidX]
y2 = validY
testX = np.concatenate((trainX, validX, testX), axis=0)
ftestX = np.concatenate((ftrainX, fvalidX, ftestX), axis=0)
testY = np.concatenate((trainY, validY, testY), axis=0)
X3 = [testX, ftestX]
y3 = testY

history = model.fit(X1, y1, batch_size=128, epochs=100, verbose=1, validation_data=(X2, y2), callbacks=[learning_rate_reduction])
pred = np.round(model.predict(X3), 0)

import pandas as pd

# Assuming 'history' contains the training history
# history = model.fit(...)

# Create a DataFrame from the training history
history_df = pd.DataFrame(history.history)

# Display the DataFrame
print(history_df)

# Extract validation accuracy, validation loss, and loss from the DataFrame
validation_accuracy = history_df['val_accuracy'].max()
validation_loss = history_df['val_loss'].min()
final_loss = history_df['loss'].iloc[-1]

# Display the results
print(f"\nValidation Accuracy: {validation_accuracy:.4f}")
print(f"Validation Loss: {validation_loss:.4f}")
print(f"Final Loss: {final_loss:.4f}")

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

print("Confusion Matrix:")
print(conf_mat)

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

# Extract TP, TN, FP, FN
TP = conf_mat[1, 1]  # True Positive
TN = conf_mat[0, 0]  # True Negative
FP = conf_mat[0, 1]  # False Positive
FN = conf_mat[1, 0]  # False Negative

# Display the results
print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1)))
print(classification_report(y3,pred))
plt.plot(history.history['accuracy'],'g--', linewidth=2, markersize=6)
plt.plot(history.history['val_accuracy'],'^k:', linewidth=2, markersize=6)

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train accuracy', 'test accuracy'], loc='upper left')
plt.grid()
plt.show()
# summarize history for loss
plt.plot(history.history['loss'],'b--', linewidth=2, markersize=6)
plt.plot(history.history['val_loss'],'^k:', linewidth=2, markersize=6)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train loss', 'test loss'], loc='upper left')
plt.grid()
plt.show()

score = model.evaluate(X3, y3,verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

!pip install scipy
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
from itertools import cycle
from scipy.interpolate import interp1d

x_test = X3
n_classes=7
lw=1
y_score =pred

### MACRO
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y3[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y3.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
# Plot ROC curves for the multiclass problem

# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
  mean_tpr += interp1d(fpr[i], tpr[i])(all_fpr)

# ... (rest of the code)

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=1)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=1)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve for skin cancer classification')
plt.legend(loc="lower right")
plt.show()

###################### RMSprop optimizer   ###################3

from keras.optimizers import RMSprop
from keras.callbacks import ReduceLROnPlateau

# Change optimizer to RMSprop
optimizer = RMSprop(lr=0.00125, epsilon=1e-8, rho=0.9)

# Compile the model
model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])

# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(
    monitor='val_acc',
    patience=2,
    verbose=1,
    factor=0.5,
    min_lr=0.0001
)

### Just for sanity check
trainX1 = np.concatenate((trainX, trainX, trainX), axis=0)
ftrainX1 = np.concatenate((ftrainX, ftrainX, ftrainX), axis=0)
trainY1 = np.concatenate((trainY, trainY, trainY), axis=0)
X1 = [trainX1, ftrainX1]
y1 = trainY1
X2 = [validX, fvalidX]
y2 = validY
testX = np.concatenate((trainX, validX, testX), axis=0)
ftestX = np.concatenate((ftrainX, fvalidX, ftestX), axis=0)
testY = np.concatenate((trainY, validY, testY), axis=0)
X3 = [testX, ftestX]
y3 = testY

history = model.fit(X1, y1, batch_size=128, epochs=100, verbose=1, validation_data=(X2, y2), callbacks=[learning_rate_reduction])
pred = np.round(model.predict(X3), 0)

import pandas as pd

# Assuming 'history' contains the training history
# history = model.fit(...)

# Create a DataFrame from the training history
history_df = pd.DataFrame(history.history)

# Display the DataFrame
print(history_df)

# Extract validation accuracy, validation loss, and loss from the DataFrame
validation_accuracy = history_df['val_accuracy'].max()
validation_loss = history_df['val_loss'].min()
final_loss = history_df['loss'].iloc[-1]

# Display the results
print(f"\nValidation Accuracy: {validation_accuracy:.4f}")
print(f"Validation Loss: {validation_loss:.4f}")
print(f"Final Loss: {final_loss:.4f}")

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

print("Confusion Matrix:")
print(conf_mat)

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

# Extract TP, TN, FP, FN
TP = conf_mat[1, 1]  # True Positive
TN = conf_mat[0, 0]  # True Negative
FP = conf_mat[0, 1]  # False Positive
FN = conf_mat[1, 0]  # False Negative

# Display the results
print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1)))
print(classification_report(y3,pred))
plt.plot(history.history['accuracy'],'g--', linewidth=2, markersize=6)
plt.plot(history.history['val_accuracy'],'^k:', linewidth=2, markersize=6)

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train accuracy', 'test accuracy'], loc='upper left')
plt.grid()
plt.show()
# summarize history for loss
plt.plot(history.history['loss'],'b--', linewidth=2, markersize=6)
plt.plot(history.history['val_loss'],'^k:', linewidth=2, markersize=6)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train loss', 'test loss'], loc='upper left')
plt.grid()
plt.show()

score = model.evaluate(X3, y3,verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

import seaborn as sns
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10,10))


ax = sns.heatmap(conf_mat, annot=True, cmap='Blues', linewidths=.9, ax=ax)

ax.set_title('Confusion Matrix with labels\n\n');

ax.set_xlabel('\nPredicted Values')

ax.set_ylabel('Actual Values ');

ax.xaxis.set_ticklabels(['0','1','2','3','4','5','6'])

ax.yaxis.set_ticklabels(['0','1','2','3','4','5','6'])

plt.show()

!pip install scipy
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
from itertools import cycle
from scipy.interpolate import interp1d

x_test = X3
n_classes=7
lw=1
y_score =pred

### MACRO
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y3[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y3.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
# Plot ROC curves for the multiclass problem

# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
  mean_tpr += interp1d(fpr[i], tpr[i])(all_fpr)

# ... (rest of the code)

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=1)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=1)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve for skin cancer classification')
plt.legend(loc="lower right")
plt.show()

########################3 ADAGRAD OPTIMIZER #############################3



from keras.optimizers import Adagrad
from keras.callbacks import ReduceLROnPlateau

# Change optimizer to Adagrad
optimizer = Adagrad(learning_rate=0.00125, epsilon=1e-8)

# Compile the model
model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])

# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(
    monitor='val_acc',
    patience=2,
    verbose=1,
    factor=0.5,
    min_lr=0.0001
)

### Just for sanity check
trainX1 = np.concatenate((trainX, trainX, trainX), axis=0)
ftrainX1 = np.concatenate((ftrainX, ftrainX, ftrainX), axis=0)
trainY1 = np.concatenate((trainY, trainY, trainY), axis=0)
X1 = [trainX1, ftrainX1]
y1 = trainY1
X2 = [validX, fvalidX]
y2 = validY
testX = np.concatenate((trainX, validX, testX), axis=0)
ftestX = np.concatenate((ftrainX, fvalidX, ftestX), axis=0)
testY = np.concatenate((trainY, validY, testY), axis=0)
X3 = [testX, ftestX]
y3 = testY

history = model.fit(X1, y1, batch_size=128, epochs=100, verbose=1, validation_data=(X2, y2), callbacks=[learning_rate_reduction])
pred = np.round(model.predict(X3), 0)

import pandas as pd

# Assuming 'history' contains the training history
# history = model.fit(...)

# Create a DataFrame from the training history
history_df = pd.DataFrame(history.history)

# Display the DataFrame
print(history_df)

# Extract validation accuracy, validation loss, and loss from the DataFrame
validation_accuracy = history_df['val_accuracy'].max()
validation_loss = history_df['val_loss'].min()
final_loss = history_df['loss'].iloc[-1]

# Display the results
print(f"\nValidation Accuracy: {validation_accuracy:.4f}")
print(f"Validation Loss: {validation_loss:.4f}")
print(f"Final Loss: {final_loss:.4f}")

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

print("Confusion Matrix:")
print(conf_mat)

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

# Extract TP, TN, FP, FN
TP = conf_mat[1, 1]  # True Positive
TN = conf_mat[0, 0]  # True Negative
FP = conf_mat[0, 1]  # False Positive
FN = conf_mat[1, 0]  # False Negative

# Display the results
print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1)))
print(classification_report(y3,pred))
plt.plot(history.history['accuracy'],'g--', linewidth=2, markersize=6)
plt.plot(history.history['val_accuracy'],'^k:', linewidth=2, markersize=6)

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train accuracy', 'test accuracy'], loc='upper left')
plt.grid()
plt.show()
# summarize history for loss
plt.plot(history.history['loss'],'b--', linewidth=2, markersize=6)
plt.plot(history.history['val_loss'],'^k:', linewidth=2, markersize=6)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train loss', 'test loss'], loc='upper left')
plt.grid()
plt.show()

score = model.evaluate(X3, y3,verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

!pip install scipy
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
from itertools import cycle
from scipy.interpolate import interp1d

x_test = X3
n_classes=7
lw=1
y_score =pred

### MACRO
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y3[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y3.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
# Plot ROC curves for the multiclass problem

# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
  mean_tpr += interp1d(fpr[i], tpr[i])(all_fpr)

# ... (rest of the code)

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=1)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=1)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve for skin cancer classification')
plt.legend(loc="lower right")
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10,10))


ax = sns.heatmap(conf_mat, annot=True, cmap='Blues', linewidths=.9, ax=ax)

ax.set_title('Confusion Matrix with labels\n\n');

ax.set_xlabel('\nPredicted Values')

ax.set_ylabel('Actual Values ');

ax.xaxis.set_ticklabels(['0','1','2','3','4','5','6'])

ax.yaxis.set_ticklabels(['0','1','2','3','4','5','6'])

plt.show()

import numpy as np

def calculate_metrics(conf_matrix):
    metrics = []

    for i in range(conf_matrix.shape[0]):
        true_positive = conf_matrix[i, i]
        false_positive = np.sum(conf_matrix[:, i]) - true_positive
        false_negative = np.sum(conf_matrix[i, :]) - true_positive
        true_negative = np.sum(conf_matrix) - true_positive - false_positive - false_negative

        metrics.append({
            'class': i,
            'true_positive': true_positive,
            'true_negative': true_negative,
            'false_positive': false_positive,
            'false_negative': false_negative
        })

    return metrics

# Example usage:
# Replace `your_conf_matrix` with your actual confusion matrix of size 7x7
your_conf_matrix = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

metrics = calculate_metrics(your_conf_matrix)

# Print the metrics for each class
for metric in metrics:
    print(f"Class {metric['class']}:")
    print(f"True Positive: {metric['true_positive']}")
    print(f"True Negative: {metric['true_negative']}")
    print(f"False Positive: {metric['false_positive']}")
    print(f"False Negative: {metric['false_negative']}\n")

##########################   ADAMAX optimizer     ###########################

from keras.optimizers import Adamax
from keras.callbacks import ReduceLROnPlateau

# Change optimizer to Adamax
optimizer = Adamax(lr=0.00125, beta_1=0.9, beta_2=0.999, epsilon=1e-8)

# Compile the model
model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])

# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(
    monitor='val_acc',
    patience=2,
    verbose=1,
    factor=0.5,
    min_lr=0.0001
)

### Just for sanity check
trainX1 = np.concatenate((trainX, trainX, trainX), axis=0)
ftrainX1 = np.concatenate((ftrainX, ftrainX, ftrainX), axis=0)
trainY1 = np.concatenate((trainY, trainY, trainY), axis=0)
X1 = [trainX1, ftrainX1]
y1 = trainY1
X2 = [validX, fvalidX]
y2 = validY
testX = np.concatenate((trainX, validX, testX), axis=0)
ftestX = np.concatenate((ftrainX, fvalidX, ftestX), axis=0)
testY = np.concatenate((trainY, validY, testY), axis=0)
X3 = [testX, ftestX]
y3 = testY

history = model.fit(X1, y1, batch_size=128, epochs=100, verbose=1, validation_data=(X2, y2), callbacks=[learning_rate_reduction])
pred = np.round(model.predict(X3), 0)

import pandas as pd

# Assuming 'history' contains the training history
# history = model.fit(...)

# Create a DataFrame from the training history
history_df = pd.DataFrame(history.history)

# Display the DataFrame
print(history_df)

# Extract validation accuracy, validation loss, and loss from the DataFrame
validation_accuracy = history_df['val_accuracy'].max()
validation_loss = history_df['val_loss'].min()
final_loss = history_df['loss'].iloc[-1]

# Display the results
print(f"\nValidation Accuracy: {validation_accuracy:.4f}")
print(f"Validation Loss: {validation_loss:.4f}")
print(f"Final Loss: {final_loss:.4f}")

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

print("Confusion Matrix:")
print(conf_mat)

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

# Extract TP, TN, FP, FN
TP = conf_mat[1, 1]  # True Positive
TN = conf_mat[0, 0]  # True Negative
FP = conf_mat[0, 1]  # False Positive
FN = conf_mat[1, 0]  # False Negative

# Display the results
print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1)))
print(classification_report(y3,pred))
plt.plot(history.history['accuracy'],'g--', linewidth=2, markersize=6)
plt.plot(history.history['val_accuracy'],'^k:', linewidth=2, markersize=6)

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train accuracy', 'test accuracy'], loc='upper left')
plt.grid()
plt.show()
# summarize history for loss
plt.plot(history.history['loss'],'b--', linewidth=2, markersize=6)
plt.plot(history.history['val_loss'],'^k:', linewidth=2, markersize=6)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train loss', 'test loss'], loc='upper left')
plt.grid()
plt.show()

score = model.evaluate(X3, y3,verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

!pip install scipy
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
from itertools import cycle
from scipy.interpolate import interp1d

x_test = X3
n_classes=7
lw=1
y_score =pred

### MACRO
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y3[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y3.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
# Plot ROC curves for the multiclass problem

# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
  mean_tpr += interp1d(fpr[i], tpr[i])(all_fpr)

# ... (rest of the code)

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=1)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=1)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve for skin cancer classification')
plt.legend(loc="lower right")
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10,10))


ax = sns.heatmap(conf_mat, annot=True, cmap='Blues', linewidths=.9, ax=ax)

ax.set_title('Confusion Matrix with labels\n\n');

ax.set_xlabel('\nPredicted Values')

ax.set_ylabel('Actual Values ');

ax.xaxis.set_ticklabels(['0','1','2','3','4','5','6'])

ax.yaxis.set_ticklabels(['0','1','2','3','4','5','6'])

plt.show()

import numpy as np

def calculate_metrics(conf_matrix):
    metrics = []

    for i in range(conf_matrix.shape[0]):
        true_positive = conf_matrix[i, i]
        false_positive = np.sum(conf_matrix[:, i]) - true_positive
        false_negative = np.sum(conf_matrix[i, :]) - true_positive
        true_negative = np.sum(conf_matrix) - true_positive - false_positive - false_negative

        metrics.append({
            'class': i,
            'true_positive': true_positive,
            'true_negative': true_negative,
            'false_positive': false_positive,
            'false_negative': false_negative
        })

    return metrics

# Example usage:
# Replace `your_conf_matrix` with your actual confusion matrix of size 7x7
your_conf_matrix = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

metrics = calculate_metrics(your_conf_matrix)

# Print the metrics for each class
for metric in metrics:
    print(f"Class {metric['class']}:")
    print(f"True Positive: {metric['true_positive']}")
    print(f"True Negative: {metric['true_negative']}")
    print(f"False Positive: {metric['false_positive']}")
    print(f"False Negative: {metric['false_negative']}\n")

########################3333   AMSGrad otimizer   ##################3



from keras.optimizers import Adam
from keras.callbacks import ReduceLROnPlateau

# Change optimizer to Adam with AMSGrad
optimizer = Adam(lr=0.00125, epsilon=1e-8, beta_1=0.9, beta_2=0.999, amsgrad=True)

# Compile the model
model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])

# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(
    monitor='val_acc',
    patience=2,
    verbose=1,
    factor=0.5,
    min_lr=0.0001
)

### Just for sanity check
trainX1 = np.concatenate((trainX, trainX, trainX), axis=0)
ftrainX1 = np.concatenate((ftrainX, ftrainX, ftrainX), axis=0)
trainY1 = np.concatenate((trainY, trainY, trainY), axis=0)
X1 = [trainX1, ftrainX1]
y1 = trainY1
X2 = [validX, fvalidX]
y2 = validY
testX = np.concatenate((trainX, validX, testX), axis=0)
ftestX = np.concatenate((ftrainX, fvalidX, ftestX), axis=0)
testY = np.concatenate((trainY, validY, testY), axis=0)
X3 = [testX, ftestX]
y3 = testY

history = model.fit(X1, y1, batch_size=128, epochs=100, verbose=1, validation_data=(X2, y2), callbacks=[learning_rate_reduction])
pred = np.round(model.predict(X3), 0)

import pandas as pd

# Assuming 'history' contains the training history
# history = model.fit(...)

# Create a DataFrame from the training history
history_df = pd.DataFrame(history.history)

# Display the DataFrame
print(history_df)

# Extract validation accuracy, validation loss, and loss from the DataFrame
validation_accuracy = history_df['val_accuracy'].max()
validation_loss = history_df['val_loss'].min()
final_loss = history_df['loss'].iloc[-1]

# Display the results
print(f"\nValidation Accuracy: {validation_accuracy:.4f}")
print(f"Validation Loss: {validation_loss:.4f}")
print(f"Final Loss: {final_loss:.4f}")

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

print("Confusion Matrix:")
print(conf_mat)

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

# Extract TP, TN, FP, FN
TP = conf_mat[1, 1]  # True Positive
TN = conf_mat[0, 0]  # True Negative
FP = conf_mat[0, 1]  # False Positive
FN = conf_mat[1, 0]  # False Negative

# Display the results
print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1)))
print(classification_report(y3,pred))
plt.plot(history.history['accuracy'],'g--', linewidth=2, markersize=6)
plt.plot(history.history['val_accuracy'],'^k:', linewidth=2, markersize=6)

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train accuracy', 'test accuracy'], loc='upper left')
plt.grid()
plt.show()
# summarize history for loss
plt.plot(history.history['loss'],'b--', linewidth=2, markersize=6)
plt.plot(history.history['val_loss'],'^k:', linewidth=2, markersize=6)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train loss', 'test loss'], loc='upper left')
plt.grid()
plt.show()

score = model.evaluate(X3, y3,verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

!pip install scipy
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
from itertools import cycle
from scipy.interpolate import interp1d

x_test = X3
n_classes=7
lw=1
y_score =pred

### MACRO
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y3[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y3.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
# Plot ROC curves for the multiclass problem

# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
  mean_tpr += interp1d(fpr[i], tpr[i])(all_fpr)

# ... (rest of the code)

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=1)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=1)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve for skin cancer classification')
plt.legend(loc="lower right")
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10,10))


ax = sns.heatmap(conf_mat, annot=True, cmap='Blues', linewidths=.9, ax=ax)

ax.set_title('Confusion Matrix with labels\n\n');

ax.set_xlabel('\nPredicted Values')

ax.set_ylabel('Actual Values ');

ax.xaxis.set_ticklabels(['0','1','2','3','4','5','6'])

ax.yaxis.set_ticklabels(['0','1','2','3','4','5','6'])

plt.show()

#######################  ADAdelta optimizer  #########################3

from keras.optimizers import Adadelta

# Change optimizer to Adadelta
optimizer = Adadelta(learning_rate=1.0, rho=0.95, epsilon=1e-08)

# Compile the model
model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])

# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(
    monitor='val_acc',
    patience=2,
    verbose=1,
    factor=0.5,
    min_lr=0.0001
)

# Rest of the code remains the same for training
### Just for sanity check
trainX1 = np.concatenate((trainX, trainX, trainX), axis=0)
ftrainX1 = np.concatenate((ftrainX, ftrainX, ftrainX), axis=0)
trainY1 = np.concatenate((trainY, trainY, trainY), axis=0)
X1 = [trainX1, ftrainX1]
y1 = trainY1
X2 = [validX, fvalidX]
y2 = validY
testX = np.concatenate((trainX, validX, testX), axis=0)
ftestX = np.concatenate((ftrainX, fvalidX, ftestX), axis=0)
testY = np.concatenate((trainY, validY, testY), axis=0)
X3 = [testX, ftestX]
y3 = testY
history = model.fit(X1, y1, batch_size=128, epochs=100, verbose=1, validation_data=(X2, y2), callbacks=[learning_rate_reduction])
pred = np.round(model.predict(X3), 0)

import pandas as pd

# Assuming 'history' contains the training history
# history = model.fit(...)

# Create a DataFrame from the training history
history_df = pd.DataFrame(history.history)

# Display the DataFrame
print(history_df)

# Extract validation accuracy, validation loss, and loss from the DataFrame
validation_accuracy = history_df['val_accuracy'].max()
validation_loss = history_df['val_loss'].min()
final_loss = history_df['loss'].iloc[-1]

# Display the results
print(f"\nValidation Accuracy: {validation_accuracy:.4f}")
print(f"Validation Loss: {validation_loss:.4f}")
print(f"Final Loss: {final_loss:.4f}")

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

print("Confusion Matrix:")
print(conf_mat)

from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming `model` is already defined and trained using the provided code

# Assuming X3 and y3 are your test set
# X3 = [testX, ftestX]
# y3 = testY

# Predict on the test set
pred = np.round(model.predict(X3), 0)

# Get the confusion matrix
conf_mat = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

# Extract TP, TN, FP, FN
TP = conf_mat[1, 1]  # True Positive
TN = conf_mat[0, 0]  # True Negative
FP = conf_mat[0, 1]  # False Positive
FN = conf_mat[1, 0]  # False Negative

# Display the results
print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1)))
print(classification_report(y3,pred))
plt.plot(history.history['accuracy'],'g--', linewidth=2, markersize=6)
plt.plot(history.history['val_accuracy'],'^k:', linewidth=2, markersize=6)

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train accuracy', 'test accuracy'], loc='upper left')
plt.grid()
plt.show()
# summarize history for loss
plt.plot(history.history['loss'],'b--', linewidth=2, markersize=6)
plt.plot(history.history['val_loss'],'^k:', linewidth=2, markersize=6)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train loss', 'test loss'], loc='upper left')
plt.grid()
plt.show()

score = model.evaluate(X3, y3,verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

!pip install scipy
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
from itertools import cycle
from scipy.interpolate import interp1d

x_test = X3
n_classes=7
lw=1
y_score =pred

### MACRO
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y3[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y3.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
# Plot ROC curves for the multiclass problem

# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
  mean_tpr += interp1d(fpr[i], tpr[i])(all_fpr)

# ... (rest of the code)

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=1)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=1)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve for skin cancer classification')
plt.legend(loc="lower right")
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10,10))


ax = sns.heatmap(conf_mat, annot=True, cmap='Blues', linewidths=.9, ax=ax)

ax.set_title('Confusion Matrix with labels\n\n');

ax.set_xlabel('\nPredicted Values')

ax.set_ylabel('Actual Values ');

ax.xaxis.set_ticklabels(['0','1','2','3','4','5','6'])

ax.yaxis.set_ticklabels(['0','1','2','3','4','5','6'])

plt.show()

import numpy as np

def calculate_metrics(conf_matrix):
    metrics = []

    for i in range(conf_matrix.shape[0]):
        true_positive = conf_matrix[i, i]
        false_positive = np.sum(conf_matrix[:, i]) - true_positive
        false_negative = np.sum(conf_matrix[i, :]) - true_positive
        true_negative = np.sum(conf_matrix) - true_positive - false_positive - false_negative

        metrics.append({
            'class': i,
            'true_positive': true_positive,
            'true_negative': true_negative,
            'false_positive': false_positive,
            'false_negative': false_negative
        })

    return metrics

# Example usage:
# Replace `your_conf_matrix` with your actual confusion matrix of size 7x7
your_conf_matrix = confusion_matrix(y3.argmax(axis=1), pred.argmax(axis=1))

metrics = calculate_metrics(your_conf_matrix)

# Print the metrics for each class
for metric in metrics:
    print(f"Class {metric['class']}:")
    print(f"True Positive: {metric['true_positive']}")
    print(f"True Negative: {metric['true_negative']}")
    print(f"False Positive: {metric['false_positive']}")
    print(f"False Negative: {metric['false_negative']}\n")

